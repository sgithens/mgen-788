\documentclass[11pt]{sigplanconf}

% The following \documentclass options may be useful:
%
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{fancyvrb}
\usepackage{multirow}
\usepackage{url}
\usepackage{hyperref}
\usepackage{breakurl}
\usepackage{caption}
\usepackage{float}
\DeclareCaptionType{copyrightbox}

\begin{document}

\CopyrightYear{2013}

\titlebanner{Wavefront Paper}        % These are ignored unless
\preprintfooter{Open Windfarm}   % 'preprint' option specified.

\title{In search of a flexible, open, and extensible visualization and query 
 facility for biobanking.}
\subtitle{Combining traditional and NGS sample data}

\authorinfo{Steven Githens}
           {IUPUI}
           {sgithens@iupui.edu}

\maketitle

\begin{abstract}
With the explosion of low cost sequencing technologies, modern biobanks are 
now facing the task of interfacing their existing sample annotation data with
large amounts of patient sequence data. Assembling internal and external facing user interfaces for 
searching, querying, and exporting of tissue data has been a fragile and tightly
coupled process. In this case report, we examine the potential of using a generalized
framework such as Harvest for adding data exploration functionality to
the Komen Tissue Bank. An emphasis is placed on merging query capabilities 
across non-sequence data and NGS derived data. 
\end{abstract}

\terms
biobank, sequencing, gene, data science

\keywords
Harvest, Komen Tissue Bank, Python, NGS

\section{Introduction}

This case study involves a medium size biobank, for our examples we use the
Komen Tissue Bank at the IU Simon Cancer center \cite{komen-tissue-bank}.  
The Komen Tissue Bank currently has roughly 3200 physical samples, 
with a tragectory of doubling in size over the next decade. The
central problem revolves around searching and aggregating data generated from
specific samples, and how to deal with an influx of sequence data that is 
becoming readily available with the drop in cost of NGS technology.  Banks such
as this may already have information about each sample available in a relational
database, but suddenly need to augment it with varying types of genomic and
proteomics data being generated from sequencing specimens in the bank.

We will look at the types of data involved with this bank, their needs and use
cases, and a promising technical infrastructure for harmonizing data retrieval on
existing infrastructure.
Additionally, we will look at future directions of this work incorporating 
other ways of federating and aggregating NGS queries across data stores.

\section{Methods and Technologies}

When looking at solutions we consider a number of of factors such as the types
of data we need to aggregate and the underlying software platforms.  Social and
community support of development are also considered.

\subsection{Categories of Data}

When developing solutions to this problem space, we tend to split our
data in to 2 categories: NGS data obtained from sequencing bank samples, and 
non-sequence data that goes with each sample.  We make this distinction for the
convenience of appropriating data storage and querying solutions. It will 
typically be the case that for most (if not all) samples, the NGS data will be 
much larger and complex in size, thusly requiring more thought for
indexing and lookup strategies.

\begin{figure}[htb]
\centering
\includegraphics[width=3.2in]{images/datasources.png}
\caption{Sources of (meta)data for each tissue sample.}
\label{fig:datasources}
\end{figure}

Figure \ref{fig:datasources} shows the general types of data that would be linked to a sample at the
Komen Tissue Bank.  The barcode acts as the unique
identifier for each set of physical samples from an encounter.  In this case an
encounter consists of a woman donating blood and several cores of breast tissue.
Though there are multiple cores, since they are from the same donation encounter
they receive the same barcode. This barcode acts as the weighted primary key for
all lookups.

The non-sequence data in this example consists of the annotation data, image
data, and available aliquots. The annotation data consists of a medical 
questionnaire with background demographics and medical history. The image data 
consists of mammogram DICOM images, and H\&E slides that are made 
from extracted tissue.  The available aliquots are how much sample tissue is
still available from the barcode, and decreases each time tissue samples are 
sent out for wet lab research.  
The latter is an exemplary case of why we need a flexible solution that
sits on top of live changing data, rather than a static viewer.

The rest is NGS data such as sequence alignment files, variant call and gene 
mutation data.  We do acknowledge that there are many other types of data and 
files that are produced by NGS sequencing, but for investigating the Harvest 
platform libraries, we start with the VCF file format as an example.  

With regard to the NGS data, there are actually several very nice web 
applications that allow the sort of exploration we are looking for.  One 
example is the cBioPortal for Cancer Genomics 
in place at the Memorial Sloan-Kettering Cancer Center.
\cite{sloan-kettering}

\begin{figure}[htb]
\centering
\includegraphics[width=3.2in]{images/cbioportal-screenshot1.png}
\caption{cBioPortal Query Home, allowing a list of genes to search and other 
cancer metadata.}
\label{fig:aggregatedexcel}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=3.2in]{images/cbioportal-screenshot2.png}
\caption{View of a single sample from cBioPortal highlighting mutations of
interest and other data available on a tabbed user interface.}
\label{fig:aggregatedexcel}
\end{figure}

The initial page of the cBioPortal allows the selection and filtering for 
various types of cancer data, as well as the input of specific genes to search 
for variants.  After specifying the query and getting a list of patients we 
can see the view for a specific barcode, from which is available all the data
linked to that sample, such as interesting gene mutations.

In reality, this is close to the kind of final view we want for our barcodes.  The major
difference is that we would like it housed in an infrastructure that allows 
easier customization and addition of metadata types to the query and view. The
issue with most sites like cBioPortal is that the addition of new tabs or panes
in the UI comes with the upfront cost of heavy software coding and customization.
We aren't completely against doing that, but would like the entire workflow 
for metadata layering to
be streamlined from the datastore query to the UI presentation.

\subsection{Technical Considerations}

To complement the data category types we like to keep in mind some other 
technical aspects as well. The first is whether we move from a single datasource to 
an aggregated or federated search model, and how this impacts our search speed.
For instance we start by operating on the assumption that all data is in a 
traditional relational database such as MySQL or Oracle, but that eventually
we may need searches to run across multiple data stores.

This goes hand in hand with search speed which we like to categorize as
syncronous, almost syncronous, and asyncronous.  For syncronous searches, we
expect them to complete in a webrowser in under 10 seconds.  For "almost
syncronous", a term we are making up for this paper, we consider searches
that a user expects to complete quickly, but instead cause
the browser to spin. Often for these the browser search can 
take upwards of one to two minutes to complete.  Asyncronous
searches are put on a queue, and the user is alerted when the search as completed.

Although we currently employ a single relational database, we keep in mind
use cases for using other NoSQL datastores, as well as making calls to various
web services such as those run by NCBI.  For this we also note that our data
accesses abundantly consist of reads rather than writes.  Writes will occur when
new patient data is entered, or aliquots available on a sample change, but these
events occur very sparsely occured to database reads.

From a cultural perspective, it would also be nice if the software was developed
by a robust open source community.  In order to ensure future flexibility and
extensiveness a strong group of camaraderie coupled with a modern software
development platform is preferable.

\section{Results}

\subsection{Applying the Harvest Framework}

After a brief review of projects and papers, a promising 
candidate was found in the recently published paper on the Harvest Framework
\cite{havest-manuscript}.  A federally funded project at the Center for Biomedical Informatics
at The Children's Hospital of Philadelphia, Harvest is an open source framework
built using Django and Python.  

The Harvest project delivers a number of programming libraries to realize
the HTML5 visualization depicted. \ref{fig:harvest-exploration}  At the data
layer, it builds a metadata schema on top of the Django ORM \cite{django-orm}, that allows 
one to specify the necessary textual labels and relationships to generate query builders
on top of relational tables.  For instance, it allows you to add 
descriptions of what unit a particular column may be in (such as inches, meters,
beats per minute, etc) or whether that information to be derived from another 
column. This is achieved using their Avocado library \cite{avocado}. Many of these
relationships can be achieved by adding configuration to their database tables,
and custom widgets can be added with code.  If you have a database schema that
uses Django, or can easily be mapped to the Django ORM \cite{django-inspectdb}, 
then this is a straightforward process.

Along with Avocado, there is a library called ModelTree \cite{modeltree} that 
calculates all the deterministic paths in a relational schema using primary and 
foreign keys that allows the data exploration to be created.  

\begin{figure}[htb]
\centering
\includegraphics[width=3.2in]{images/harvest-screenshot.png}
\caption{Open exploration of medical data using Harvest.}
\label{fig:harvest-exploration}
\end{figure}

While this all sounds very abstract, the end result is that with just a small 
amount of metadata, one can create these data query UI's and exploration tools
with just a small amount of metadata on top of their existing relational 
database schema. It is worth spending a few minutes with the online demo to see
 the result. \cite{harvest-demo}

\subsection{VCF Format Example}

Using the VCF \cite{vcf-format} as a starting example, we can add a relational table containing 
columns for the fields in the VCF, plus a column for the barcode that the 
sequence data was derived from. It would also be useful to do some extra work
on the VCF during import of the data to make queries and indexes more useful.

For example, we can add another column called variant\_type to take the value
of SNP, Indel, or another variant type.  Our script that imports VCF data would 
then look at the REF and ALT fields and set the variant\_type to SNP or Indel
based on the two values. (ie. "G" $\to$ "C" would be SNP).  With just this extra 
column we can begin to use the built in Harvest widgets and tooling to 
filter for variants with a reference and particular variant type.

The development workflow for this is as follows.  After specifying a model, 
such as our new VCF table, Harvest contains a utility that will search your
models and add their metadata to the Avocado tables using default values 
available from your ORM mapping. If additional tweaks or labels are needed, one
can then update the metadata table rows, or programmatically create DataField
and Concept objects using Python code.

\section{Discussion}

\subsection{Future directions and data stores}

If we think about the metrics from the previous section, we may become uncertain
of how long this type of data storage will be scalable for a single instance of a 
relational database.  For the VCF example, with just 3200 sequenced samples, and
several million SNPs per sample we already a large amount of data.  Any additional
formats, and the potential desire to reference the individual's sequence 
alignment data would add to this. 

At some point, for performance reasons, we may want to consider either sharding on
several relational databases, or sharding upon multiple datastores. Some of these
being web service endpoints or another type of document, key-value, or other
NoSQL database.

For example, using a document oriented datastore such as MongoDB, we could easily
import the VCF data as a small JSON format, and then shard Mongo instances by
patient. Searching over all the nodes with a MapReduce query would could produce
favorable results.  

This stage of the project is currently under planning and being discussed on the
Harvest discussion forums. \cite{harvest-nosql-discussion}

\subsection{Conclusion}

Based on the criteria above, it's likely we will be using various Harvest 
subprojects, along with a set of utlity scripts to bootstrap querying and 
retrieval of our current non-sequence data and to interface with our sequence
data as it is added to our sample data.  Additionally, performance testing 
against the NGS data and it's storage engines will be done in an iterative 
fashion as more and more data becomes available to the bank.


%
%\begin{itemize}
%\item Date, separated into day and minute columns.
%\item Wind Speed, in meters per second, measured at 80 meters above the 
%ground.
%\item Netpower, in MegaWatts.
%\end{itemize}


%\begin{figure}
%\fontsize{7pt}{8pt}\selectfont
%\begin{verbatim}
%SITE NUMBER: 00001 RATED CAP:  171.8 IEC CLASS: 1 LOSSES (%): 14.2
%SITE LATITUDE:   34.98420 LONGITUDE: -104.03971
%DATE,TIME(UTC),SPEED80M(M/S),NETPOWER(MW)
%20040101,0010,6.60,46.34
%20040101,0020,6.77,48.00
%20040101,0030,7.17,53.37
%20040101,0040,7.84,62.71
%20040101,0050,8.97,79.92
%\end{verbatim}
%\caption{Raw data structure}
%\label{fig:rawdatastructure}
%\end{figure}
%
%
%\subsection{Looking at a particular day}
%
%To get a better feeling of what a day of wind data looks like, 
%we can use R to parse a file for a single day, and then pull
%just one days worth of data from that file. \footnote{See the plotDay
%function in the R source}
%

%
%\begin{figure}[htb]
%\centering
%\includegraphics[width=3.2in]{images/2004-01-01-time-vs-power.png}
%\caption{Time vs Power for a Single Day}
%\label{fig:timevspower}
%\end{figure}
%
%From the graphs of time vs speed \ref{fig:timevsspeed} and 
%time vs power \ref{fig:timevspower} we can get a feel of how
%the speed affects power as the curves mimic each other to
%a certain extent.
%
%\subsection{Collecting data from all Site Files}
%
%In addition to working on single site csv files, we may
%want to collect and process header information from all 
%of them. This would be necessary for reproducing the final
%aggregated data as part of the next section, but also for
%other visual means.
%
%The python source shows how we can parse all the site csv files we
%have from the simulated study, and then output them in a format
%suitable for creating a geographic visualization.  In this example we
%are creating another csv file that can be used as input to BatchGEO,
%and popular Web 2.0 site that allows overlaying site data on a Google
%Map or Open Streetmap style mashup. \ref{fig:onshoremap} \footnote{The
%  output from this data run is located at
%  \url{http://batchgeo.com/map/c0bb3d2534dc11a0ac8e303b5cff809e}}
%Currently we are just pulling out the site number, latitude, and longtitude, but any
%of the header items could be pulled out.
%
%\begin{figure}[htb]
%\centering
%\includegraphics[width=3.2in]{images/onshore-sites-map.png}
%\caption{Interactive map of onshore sites}
%\label{fig:onshoremap}
%\end{figure}
%
%\section{Reproducing the Data}
%
%\subsection{A Small Excercise in Reverse Engineering Data}

%
%Given a current lack of access to civil or mechanical engineers
%we will bootstrap \footnote{It's important to note that in this 
%scenerio, we use the term bootstrap in it's software engineering
%context, not in the context of statistical bootstrapping that 
%invovles taking subsamples of your existing data, although we
%are doing work based on subsets of the data.}
%our model by building it based off of the {\it current}
%data from the study. Given the incredibly large number of samples
%, this has turned out to be a reasonable approach.

%
%\subsection{Simulating the Wind Speeds}
%
%The problem of simulating wind speeds can be solved by using 
%an open software package that was used for generating the
%analogous data set for the western half of the United States.
%The Weather Research \& Forecasting Model \cite{wrfhome} is a collaborative
%project among a number of universities to build a set of
%tools for predictive analysis and simulation of weather
%and atmospheric conditions.
%
%The project is written in C and Fortran and relatively straightforward
%to compile on a modern Linux distribution such as Ubuntu or
%Fedora. Being a rather large project, there is some learning curve, so
%for this study we did not get much further than downloading the source
%\cite{wrfdownload}, compiling, and looking a few of the tutorials
%\cite{wrftutorial}. However, given adequate resource, the development
%of the configuration files and sample data for simulating wind speed
%samples on the eastern half of the continent is certainly feasible.
%
%\subsection{Looking for a Turbine Curve Model}
%
%We want to create a model for the turbine curve based off of
%wind speed. For sake of computation speed, we will use
%1 days worth of data from site 1 for these figures. 
%\footnote{Also because when plotting large amounts of the data,
%the curve becomes very dense from the points, making it hard
%to see other overlays and trends in the data.}
%
%A beginning scatter plot shows that the data is a bit like a 
%cubic on it's side, and we may want to see if we can still
%fit a linear model to it. It's clear that a first order model
%will be an awful fit, but we can come close with a cubic 
%model. \ref{fig:linearfit}
%
%\begin{figure}[htb]
%\centering
%\includegraphics[width=3.2in]{images/linearFit.png}
%\caption{First attempt with cubic linear model}
%\label{fig:linearfit}
%\end{figure}
%
%
%This may be Ok, but it justs feels wrong. We {\it know} that this
%model was developed based on real turbine engines, and by 
%doing a small bit research \cite{turbinecurves}
%we find that it's well known that these \footnote{As well as many
%other other engineering problems} can be modelled with 
%Weibull distributions, so it makes sense to explore that.
%
%We can do some exploration and manually fit a Weibull over our data by
%giving it the appropriate X and Y offsets, using a shape parameter
%of roughly 1.8, and keeping alpha at 1. \ref{fig:weibullexplore} We
%can now turn to R's nls function for fitting non linear models.
%Unlike the linear model fitting in R, rather than providing a number
%of coeffients on the right hand side to fit, we provide an equation
%with certain unbound variable parameters, and these are what R will
%fit to solve the minimization problem.
%
%\begin{figure}[htb]
%\centering
%\includegraphics[width=3.2in]{images/weibullexploration.png}
%\caption{Manual inspection of a weibull to our data}
%\label{fig:weibullexplore}
%\end{figure}
%
%
%Because NLS works by iterating along a gradient problem to find the
%minimization, it is necessary to provide starting values for the
%equation. This can be a very tricky problem, and as Peter Dalgaard
%describes, ``Finding starting values is an art rather than a craft'' \cite{dalgaard}
%Given the author's current amatuer statistical abilities, we were unable to
%select starting values, but all is not lost!  R ships with several
%{\it self starting} model functions that can be used in the nls
%function. When a self starting model is used, it is able to guess
%values to start with. In the case of this model, it works very well,
%with the fitted weibull \ref{fig:weibullfit} now modelling the turbine
%output curve quite well.
%
%\begin{figure}[htb]
%\centering
%\includegraphics[width=3.2in]{images/SSweibullFit.png}
%\caption{Nonlinear Weibull Model Fit}
%\label{fig:weibullfit}
%\end{figure}
%
%From the output \ref{fig:nlsoutput} we can see the values of the 
%parameter fits for the nonlinear model, and that it only
%took 4 iterations to minimize the problem using the self
%starting weibull function.
%
%\begin{figure}
%\fontsize{7pt}{8pt}\selectfont
%\begin{verbatim}
%Formula: NETPOWER.MW. ~ SSweibull(SPEED80M.M.S., Asym, Drop, lrc, pwr)
%
%Parameters:
%     Estimate Std. Error t value Pr(>|t|)    
%Asym 163.0551     0.8997  181.23   <2e-16 ***
%Drop 116.3540     4.3543   26.72   <2e-16 ***
%lrc  -13.7634     1.0135  -13.58   <2e-16 ***
%pwr    5.5966     0.4022   13.92   <2e-16 ***
%---
%Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 
%
%Residual standard error: 5.839 on 139 degrees of freedom
%
%Number of iterations to convergence: 4 
%Achieved convergence tolerance: 4.451e-06
%\end{verbatim}
%\caption{Fitted NLS Values}
%\label{fig:nlsoutput}
%\end{figure}
%


\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\begin{thebibliography}{}
\softraggedright

\vfill\eject

\bibitem{komen-tissue-bank}
Komen Tissue Bank at the IU Simon Cancer Center, \url{http://komentissuebank.iu.edu/}

\bibitem{havest-manuscript}
Pennington JW, Ruth B, Italia MJ, et al. Harvest: an open platform for developing web-based biomedical data discovery and reporting applications.  J Am Med Inform Assoc doi:10.1136/amiajnl-2013-001825 

\bibitem{sloan-kettering}
cBioPortal for Cancer Genomics at the Memorial Sloan-Kettering Cancer Center 
\url{http://www.cbioportal.org/public-portal/}

\bibitem{django-orm}
Django ORM Reference, \url{https://docs.djangoproject.com/en/1.6/topics/db/models/}

\bibitem{avocado}
Avocado Metadata APIs, \url{http://avocado.harvest.io/doc/introduction.html}

\bibitem{django-inspectdb}
Django Legacy DB Documentation, \url{https://docs.djangoproject.com/en/1.6/howto/legacy-databases/}

\bibitem{modeltree}
ModelTree, Dynamic QuerySet Generation without joins, \url{http://modeltree.harvest.io/}

\end{thebibliography}

\end{document}
